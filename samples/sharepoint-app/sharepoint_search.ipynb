{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e0d5e2",
   "metadata": {},
   "source": [
    "\n",
    "# SharePoint Search Pipeline (Graph + Azure OpenAI + Azure AI Search)\n",
    "\n",
    "This notebook ingests content from **SharePoint via Microsoft Graph Search**, chunks & embeds it with **Azure OpenAI**, indexes it into **Azure AI Search**, and lets you run **hybrid search**.\n",
    "\n",
    "> Fill the config in the next cell (Tenant/App details and Azure keys). Then run cells from top to bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af3e4b",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Azure AD App (application permissions):**\n",
    "  - `SearchQuery.All` (Graph)\n",
    "  - `Sites.Read.All` (Graph)\n",
    "  - Optionally `Files.Read.All` if you need file bodies\n",
    "  - **Admin consent granted**\n",
    "- **Azure OpenAI** with an embeddings deployment (e.g., `text-embedding-3-large`)\n",
    "- **Azure AI Search** (vector-enabled service)\n",
    "\n",
    "> ⚠️ This notebook uses `requests` against REST APIs (Graph + Azure AI Search). No SDKs required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502633d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally, uncomment to install requirements\n",
    "# %pip install msal python-dotenv requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b8a6f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8701e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# --- Microsoft Entra ID / Graph ---\n",
    "TENANT_ID = os.getenv(\"TENANT_ID\", \"YOUR_TENANT_ID\")\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\", \"YOUR_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\", \"YOUR_CLIENT_SECRET\")\n",
    "GRAPH_SCOPE = os.getenv(\"GRAPH_SCOPE\", \"https://graph.microsoft.com/.default\")\n",
    "\n",
    "# --- Azure OpenAI (Embeddings) ---\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://YOUR-RESOURCE.openai.azure.com\")\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\", \"YOUR_AOAI_KEY\")\n",
    "AZURE_OPENAI_EMBED_DEPLOY = os.getenv(\"AZURE_OPENAI_EMBED_DEPLOY\", \"text-embedding-3-large\")\n",
    "\n",
    "# --- Azure AI Search ---\n",
    "AI_SEARCH_ENDPOINT = os.getenv(\"AI_SEARCH_ENDPOINT\", \"https://YOUR-SEARCH.search.windows.net\")\n",
    "AI_SEARCH_KEY = os.getenv(\"AI_SEARCH_KEY\", \"YOUR_SEARCH_KEY\")\n",
    "AI_SEARCH_INDEX = os.getenv(\"AI_SEARCH_INDEX\", \"sp-hybrid-index\")\n",
    "\n",
    "# --- Chunking ---\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"1500\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"200\"))\n",
    "\n",
    "print(\"Config loaded. Edit values above or set environment variables before running.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b16d06",
   "metadata": {},
   "source": [
    "## Auth & HTTP helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8890e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import msal, requests\n",
    "\n",
    "def get_graph_token(tenant_id, client_id, client_secret, scope):\n",
    "    app = msal.ConfidentialClientApplication(\n",
    "        client_id=client_id,\n",
    "        client_credential=client_secret,\n",
    "        authority=f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "    )\n",
    "    result = app.acquire_token_silent([scope], account=None)\n",
    "    if not result:\n",
    "        result = app.acquire_token_for_client(scopes=[scope])\n",
    "    if \"access_token\" not in result:\n",
    "        raise RuntimeError(f\"Graph token error: {result}\")\n",
    "    return result[\"access_token\"]\n",
    "\n",
    "def graph_get(url, token, **kwargs):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    return requests.get(url, headers=headers, **kwargs)\n",
    "\n",
    "def graph_post(url, token, json):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    return requests.post(url, headers=headers, json=json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62506c4",
   "metadata": {},
   "source": [
    "## Microsoft Graph SharePoint Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b101fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GRAPH_SEARCH_URL = \"https://graph.microsoft.com/v1.0/search/query\"\n",
    "\n",
    "def sp_search(query=\"*\", size=100):\n",
    "    token = get_graph_token(TENANT_ID, CLIENT_ID, CLIENT_SECRET, GRAPH_SCOPE)\n",
    "    body = {\n",
    "        \"requests\": [{\n",
    "            \"entityTypes\": [\"driveItem\",\"listItem\",\"site\",\"list\"],\n",
    "            \"query\": {\"queryString\": query},\n",
    "            \"contentSources\": [\"sharepoint\"],\n",
    "            \"from\": 0,\n",
    "            \"size\": min(size, 500)\n",
    "        }]\n",
    "    }\n",
    "    resp = graph_post(GRAPH_SEARCH_URL, token, json=body)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    hits = []\n",
    "    for res in data.get(\"value\", []):\n",
    "        for h in res.get(\"hitsContainers\", []):\n",
    "            hits += h.get(\"hits\", [])\n",
    "    return hits\n",
    "\n",
    "# Quick smoke test (requires valid credentials):\n",
    "# hits = sp_search(\"policy\", size=5)\n",
    "# len(hits), hits[0].keys() if hits else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c0e064",
   "metadata": {},
   "source": [
    "## Chunking utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def split_text(text, chunk_size=1500, overlap=200):\n",
    "    tokens = re.split(r\"(\\n\\n|\\n|\\.|\\?|!)\", text)\n",
    "    buf, cur = [], 0\n",
    "    for t in tokens:\n",
    "        piece = t if t is not None else \"\"\n",
    "        if cur + len(piece) > chunk_size and buf:\n",
    "            yield \"\".join(buf)\n",
    "            buf = [piece[-overlap:]] if overlap else []\n",
    "            cur = len(buf[0]) if buf else 0\n",
    "        else:\n",
    "            buf.append(piece)\n",
    "            cur += len(piece)\n",
    "    if buf:\n",
    "        yield \"\".join(buf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7598b09",
   "metadata": {},
   "source": [
    "## Azure OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_texts(texts):\n",
    "    url = f\"{AZURE_OPENAI_ENDPOINT.rstrip('/')}\"\n",
    "    url += f\"/openai/deployments/{AZURE_OPENAI_EMBED_DEPLOY}/embeddings?api-version=2024-02-01\"\n",
    "    headers = {\"api-key\": AZURE_OPENAI_KEY, \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"input\": texts}\n",
    "    r = requests.post(url, headers=headers, json=payload)\n",
    "    r.raise_for_status()\n",
    "    return [d[\"embedding\"] for d in r.json()[\"data\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0326c5",
   "metadata": {},
   "source": [
    "## Azure AI Search client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daa88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def ai_search_headers():\n",
    "    return {\"Content-Type\": \"application/json\", \"api-key\": AI_SEARCH_KEY}\n",
    "\n",
    "def ai_search_create_index_if_missing():\n",
    "    url = f\"{AI_SEARCH_ENDPOINT.rstrip('/')}/indexes/{AI_SEARCH_INDEX}?api-version=2024-07-01\"\n",
    "    schema = {\n",
    "      \"name\": AI_SEARCH_INDEX,\n",
    "      \"fields\": [\n",
    "        {\"name\":\"id\",\"type\":\"Edm.String\",\"key\":True,\"searchable\":False},\n",
    "        {\"name\":\"title\",\"type\":\"Edm.String\",\"searchable\":True},\n",
    "        {\"name\":\"content\",\"type\":\"Edm.String\",\"searchable\":True},\n",
    "        {\"name\":\"url\",\"type\":\"Edm.String\",\"searchable\":False},\n",
    "        {\"name\":\"siteDomain\",\"type\":\"Edm.String\",\"filterable\":True},\n",
    "        {\"name\":\"fileType\",\"type\":\"Edm.String\",\"filterable\":True},\n",
    "        {\"name\":\"vector\",\"type\":\"Collection(Edm.Single)\",\"searchable\":True,\n",
    "         \"vectorSearchDimensions\":1536,\"vectorSearchProfileName\":\"vprof\"}\n",
    "      ],\n",
    "      \"vectorSearch\": {\"profiles\":[{\"name\":\"vprof\",\"algorithm\":\"hnsw\"}]}\n",
    "    }\n",
    "    r = requests.put(url, headers=ai_search_headers(), data=json.dumps(schema))\n",
    "    if r.status_code not in (200,201,204,409):\n",
    "        r.raise_for_status()\n",
    "\n",
    "def ai_search_upsert(docs):\n",
    "    url = f\"{AI_SEARCH_ENDPOINT.rstrip('/')}/indexes/{AI_SEARCH_INDEX}/docs/index?api-version=2024-07-01\"\n",
    "    payload = {\"value\": [{\"@search.action\":\"mergeOrUpload\", **d} for d in docs]}\n",
    "    r = requests.post(url, headers=ai_search_headers(), data=json.dumps(payload))\n",
    "    r.raise_for_status()\n",
    "\n",
    "def ai_search_search(query, top=8, filters=None, vector=None):\n",
    "    url = f\"{AI_SEARCH_ENDPOINT.rstrip('/')}/indexes/{AI_SEARCH_INDEX}/docs/search?api-version=2024-07-01\"\n",
    "    body = {\"search\": query, \"top\": top}\n",
    "    if vector is not None:\n",
    "        body[\"vectorQueries\"] = [{\"vector\": vector, \"kNearestNeighbors\": top, \"fields\": \"vector\"}]\n",
    "    if filters:\n",
    "        clauses = []\n",
    "        for k,v in filters.items():\n",
    "            if isinstance(v, str):\n",
    "                clauses.append(f\"{k} eq '{v}'\")\n",
    "        if clauses:\n",
    "            body[\"filter\"] = \" and \".join(clauses)\n",
    "    r = requests.post(url, headers=ai_search_headers(), data=json.dumps(body))\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"value\", [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0e00e",
   "metadata": {},
   "source": [
    "## Ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "\n",
    "def fetch_text_from_hit(hit):\n",
    "    # Prefer summary; download if a direct link is present and accessible as text.\n",
    "    props = hit.get(\"resource\", {})\n",
    "    text = props.get(\"summary\") or \"\"\n",
    "    download = props.get(\"downloadUrl\")\n",
    "    if download:\n",
    "        try:\n",
    "            r = requests.get(download, timeout=10)\n",
    "            if r.ok and r.headers.get(\"Content-Type\",\"\").startswith((\"text/\", \"application/json\")):\n",
    "                text = r.text\n",
    "        except Exception:\n",
    "            pass\n",
    "    return text\n",
    "\n",
    "def build_doc_records(hits):\n",
    "    docs, chunks = [], []\n",
    "    for h in hits:\n",
    "        props = h.get(\"resource\", {})\n",
    "        url = props.get(\"webUrl\") or props.get(\"webUrlPreview\") or \"\"\n",
    "        title = props.get(\"name\") or props.get(\"title\") or \"Untitled\"\n",
    "        fileType = (props.get(\"fileType\") or props.get(\"fileExtension\") or \"\").lower()\n",
    "        siteDomain = url.split(\"/\")[2] if \"//\" in url else \"\"\n",
    "        body_text = fetch_text_from_hit(h)\n",
    "\n",
    "        for c in split_text(body_text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "            doc_id = hashlib.sha1((url + c).encode()).hexdigest()\n",
    "            rec = {\n",
    "                \"id\": doc_id,\n",
    "                \"title\": title,\n",
    "                \"content\": c,\n",
    "                \"url\": url,\n",
    "                \"fileType\": fileType,\n",
    "                \"siteDomain\": siteDomain\n",
    "            }\n",
    "            docs.append(rec)\n",
    "            chunks.append(c)\n",
    "    return docs, chunks\n",
    "\n",
    "def ingest_sharepoint(query=\"*\", size=100):\n",
    "    ai_search_create_index_if_missing()\n",
    "    hits = sp_search(query=query, size=size)\n",
    "    if not hits:\n",
    "        return {\"indexed\": 0}\n",
    "    docs, chunks = build_doc_records(hits)\n",
    "    if not chunks:\n",
    "        return {\"indexed\": 0}\n",
    "    vectors = embed_texts(chunks)\n",
    "    for d, v in zip(docs, vectors):\n",
    "        d[\"vector\"] = v\n",
    "    ai_search_upsert(docs)\n",
    "    return {\"indexed\": len(docs)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47d351",
   "metadata": {},
   "source": [
    "## Search convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed20088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hybrid_search(query, top_k=8, filters=None):\n",
    "    qvec = embed_texts([query])[0]\n",
    "    results = ai_search_search(query, top=top_k, filters=filters, vector=qvec)\n",
    "    def shape(r):\n",
    "        return {\n",
    "            \"score\": r.get(\"@search.score\", 0.0),\n",
    "            \"title\": r.get(\"title\",\"Untitled\"),\n",
    "            \"snippet\": (r.get(\"content\",\"\")[:320] + (\"…\" if len(r.get(\"content\",\"\"))>320 else \"\")),\n",
    "            \"url\": r.get(\"url\",\"\"),\n",
    "            \"fileType\": r.get(\"fileType\",\"\"),\n",
    "            \"siteDomain\": r.get(\"siteDomain\",\"\")\n",
    "        }\n",
    "    return list(map(shape, results))\n",
    "\n",
    "# Example (after ingest):\n",
    "# hybrid_search(\"expense reimbursement policy\", top_k=5, filters={\"fileType\":\"pdf\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d6d76",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Create/ensure index exists, then ingest some SharePoint content\n",
    "# result = ingest_sharepoint(query=\"*\", size=100)\n",
    "# result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Try a search\n",
    "# results = hybrid_search(\"travel expense policy\", top_k=5, filters=None)\n",
    "# for r in results:\n",
    "#     print(f\"[{r['score']:.2f}] {r['title']} — {r['url']}\")\n",
    "#     print(r['snippet'])\n",
    "#     print()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}